ASUS Router News Automation
é€™æ˜¯ä¸€å€‹ç«¯åˆ°ç«¯çš„è‡ªå‹•åŒ–é¢è©¦å°ˆæ¡ˆã€‚ç›®æ¨™æ˜¯çˆ¬å– ASUS Router ç›¸é—œè³‡å®‰æ–°èï¼Œå­˜å…¥ MySQL è³‡æ–™åº«ï¼Œä¸¦è‡ªå‹•å¡«å¯«è‡³ Google è¡¨å–®ã€‚

ğŸ›  æŠ€è¡“å †ç–Š (Tech Stack)
- **Language**: Python 3.14.1
- **Database**: MySQL 8.0 (Dockerized)
- **Scraper**: Requests + BeautifulSoup
- **Automation**: Selenium WebDriver
- **Infrastructure**: Docker & Docker Compose

ğŸš€ ç’°å¢ƒå»ºç½® (Windows é–‹ç™¼ç’°å¢ƒ)
ç¬¬ 0 éšæ®µï¼šå®‰è£ç·¨è¼¯å™¨ (Cursorã€Docker)
1.ä¸‹è¼‰ Cursor ä½œç‚ºç¨‹å¼ç¢¼ç·¨è¼¯å™¨
  å‰å¾€ Cursor å®˜ç¶²ï¼Œä¸‹è¼‰å®‰è£æª”ã€‚
  åŸ·è¡Œå®‰è£ç¨‹å¼ï¼Œä¸¦ä¾ç…§æŒ‡ç¤ºå®Œæˆå®‰è£ã€‚

2.ä¸‹è¼‰Docker
  å‰å¾€ Docker å®˜ç¶²ï¼Œä¸‹è¼‰å®‰è£æª”ã€‚
  åŸ·è¡Œå®‰è£ç¨‹å¼ï¼Œä¸¦ä¾ç…§æŒ‡ç¤ºå®Œæˆå®‰è£ã€‚

------------------------------------------------------------------------------------------------
â€»å¦‚æœä½ æ˜¯ç¬¬ä¸€æ¬¡åœ¨ Windows ä¸ŠåŸ·è¡Œæ­¤å°ˆæ¡ˆï¼Œè«‹ä¾ç…§ä»¥ä¸‹æ­¥é©Ÿè¨­å®š Python è™›æ“¬ç’°å¢ƒã€‚

ç¬¬ 1 éšæ®µï¼šåœ¨é›»è…¦ä¸Šå®‰è£ Python (å¼•æ“)
1. é–‹å•Ÿ Cursor çš„çµ‚ç«¯æ©Ÿï¼Œæª¢æŸ¥pythonæ˜¯å¦å·²å®‰è£
    python --version æˆ– py --version æˆ– python3 --version

2.ä¸‹è¼‰èˆ‡å®‰è£
  å‰å¾€ Python å®˜ç¶²ä¸‹è¼‰é é¢ã€‚
  é»æ“Šé»ƒè‰²æŒ‰éˆ• Download Python 3.x.xã€‚
  â€»åŸ·è¡Œä¸‹è¼‰çš„å®‰è£æª” (âš ï¸ é‡è¦)
  å‹™å¿…å‹¾é¸æœ€ä¸‹æ–¹çš„ â˜‘ï¸ Add Python.exe to PATH (å°‡ Python åŠ å…¥ç’°å¢ƒè®Šæ•¸)ã€‚
  é»é¸ Install Now å®Œæˆå®‰è£ã€‚

3.æ‰“é–‹ Cursor
  é»æ“Šå·¦å´é‚Šæ¬„çš„ã€Œæ–¹å¡Šåœ–ç¤ºã€ (Extensions)ã€‚
  æœå°‹ Pythonã€‚
  æ‰¾åˆ°ç”± Microsoft é–‹ç™¼çš„é‚£å€‹ï¼ˆé€šå¸¸ä¸‹è¼‰é‡æœ€é«˜ï¼‰ï¼Œé»æ“Š Installã€‚ (é€™å€‹å¥—ä»¶æœƒå¹«ä½ åšèªæ³•é«˜äº®ã€ç¨‹å¼ç¢¼è£œå…¨ã€é‚„èƒ½å¹«ä½ é¸è™›æ“¬ç’°å¢ƒ)

4.å®‰è£å®Œæˆå¾Œå›åˆ°æ­¥é©Ÿ1ï¼Œæª¢æŸ¥æ˜¯å¦å®‰è£æˆåŠŸï¼Œå®‰è£æˆåŠŸå¾Œï¼Œæ¥çºŒ5.é–‹å•Ÿå°ˆæ¡ˆè³‡æ–™å¤¾ã€‚

5.é–‹å•Ÿå°ˆæ¡ˆè³‡æ–™å¤¾
  åœ¨é›»è…¦æ¡Œé¢æˆ–ä½ ç¿’æ…£çš„åœ°æ–¹ï¼Œå»ºç«‹ä¸€å€‹æ–°è³‡æ–™å¤¾ï¼Œå‘½åç‚º asus-newsã€‚
  åœ¨ Cursor ä¸­ï¼Œé»é¸ File -> Open Folderï¼Œé¸æ“‡é€™å€‹è³‡æ–™å¤¾ã€‚

------------------------------------------------------------------------------------------------
ç¬¬ 2 éšæ®µï¼šå»ºç«‹è™›æ“¬ç’°å¢ƒ (Virtual Environment) - ç‚ºé¿å…å½±éŸ¿é›»è…¦å…¶ä»–å°ˆæ¡ˆï¼Œéœ€è¦å»ºç«‹ä¸€å€‹ç¨ç«‹çš„ç’°å¢ƒã€‚
1.é–‹å•Ÿ Cursor çš„çµ‚ç«¯æ©Ÿ
  ä½¿ç”¨å¿«æ·éµ Ctrl + ` é–‹å•Ÿçµ‚ç«¯æ©Ÿã€‚
  ç¢ºä¿çµ‚ç«¯æ©Ÿè·¯å¾‘æ˜¯åœ¨é€™å€‹å°ˆæ¡ˆçš„è³‡æ–™å¤¾åº•ä¸‹ã€‚

2.è«‹ä¾åºè¼¸å…¥ä»¥ä¸‹æŒ‡ä»¤ï¼š
  Windows:
  (1). å»ºç«‹è™›æ“¬ç’°å¢ƒ (åªéœ€åšä¸€æ¬¡)
       python -m venv .venv
  (2). å•Ÿå‹•è™›æ“¬ç’°å¢ƒ (æ¯æ¬¡é‡é–‹ Cursor éƒ½è¦ç¢ºèªå‰é¢æœ‰ (.venv) å­—æ¨£ï¼Œé€šå¸¸ Cursor æœƒè‡ªå‹•åµæ¸¬)
       .venv\Scripts\activate

  Mac / Linux:
  (1). å»ºç«‹è™›æ“¬ç’°å¢ƒ
       python3 -m venv .venv
  (2). å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
       source .venv/bin/activate

è¨»:
Q:å¦‚æœé‡åˆ°.venv\Scripts\activateéŒ¯èª¤ç‚ºWindows PowerShell å®‰å…¨æ€§é™åˆ¶å•é¡Œ
A:è§£æ±ºæ–¹æ³•
  æ­¥é©Ÿ 1ï¼šä¿®æ”¹åŸ·è¡Œæ¬Šé™
  Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
  æ­¥é©Ÿ 2ï¼šæ¬Šé™æ”¹å¥½å¾Œï¼Œå†åŸ·è¡Œä¸€æ¬¡åŸæœ¬çš„æŒ‡ä»¤
ğŸ’¡å¦‚ä½•ç¢ºèªæˆåŠŸï¼Ÿ
   çœ‹åˆ° Terminal çš„æœ€å‰é¢å‡ºç¾äº†ç¶ è‰²æˆ–ç™½è‰²çš„ (.venv) å­—æ¨£

------------------------------------------------------------------------------------------------
ç¬¬ 3 éšæ®µï¼šé©—è­‰èˆ‡å®‰è£ä¾è³´
1.åœ¨ Cursor å·¦å´æª”æ¡ˆç¸½ç®¡æŒ‰å³éµ -> New File -> å‘½åç‚º requirements.txtã€‚

2.è²¼ä¸Šä»¥ä¸‹å…§å®¹:
  requests>=2.31.0
  beautifulsoup4>=4.12.0
  selenium>=4.16.0
  webdriver-manager>=4.0.1
  mysql-connector-python>=8.2.0
  python-dotenv>=1.0.0
  pandas>=2.1.0

3.å›åˆ°çµ‚ç«¯æ©Ÿï¼Œè¼¸å…¥å®‰è£æŒ‡ä»¤ï¼š
  pip install -r requirements.txt

------------------------------------------------------------------------------------------------
ç¬¬ 4 éšæ®µï¼šDocker åŒ–
åœ¨ Docker ç’°å¢ƒä¸­ï¼Œæˆ‘å€‘æœƒå®šç¾©å…©å€‹ä¸»è¦çš„ Serviceï¼š
db: MySQL 8.0 è³‡æ–™åº«ã€‚
app: ä¹‹å¾Œè¦è·‘ Python çˆ¬èŸ²çš„å®¹å™¨ (ç›®å‰æˆ‘å€‘å…ˆé ç•™è¨­å®šï¼Œé‡é»å…ˆè®“ DB è·‘èµ·ä¾†)ã€‚

1.å»ºç«‹è³‡æ–™åº«åˆå§‹åŒ–è…³æœ¬ (init.sql)
  (1)åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„å»ºç«‹ä¸€å€‹è³‡æ–™å¤¾ï¼Œå‘½åç‚º dbã€‚

  (2)åœ¨ db è³‡æ–™å¤¾å…§å»ºç«‹ä¸€å€‹æª”æ¡ˆ init.sqlã€‚

  (3)è²¼ä¸Šä»¥ä¸‹ SQL ç¢¼(2å€‹TABLE)ï¼š
    -- db/init.sql

    -- 1. å»ºç«‹ä¸¦ä½¿ç”¨è³‡æ–™åº«
    CREATE DATABASE IF NOT EXISTS security_news;
    USE security_news;

    -- 2. å»ºç«‹æ–°èè³‡æ–™è¡¨
    CREATE TABLE IF NOT EXISTS news (
        id INT AUTO_INCREMENT PRIMARY KEY,
        title VARCHAR(255) NOT NULL,
        publish_date DATE NOT NULL,
        url VARCHAR(2048) NOT NULL,
        source VARCHAR(100),
        
        -- æ–°å¢: æ–°èæ‘˜è¦ (å°æ‡‰çˆ¬èŸ²çš„ description)
        description TEXT,
        
        -- æµç¨‹åœ–é‚è¼¯æ ¸å¿ƒæ¬„ä½:
        -- status: 'N'=æ–°è³‡æ–™(New), 'Y'=å·²å¡«è¡¨(Yes), 'E'=éŒ¯èª¤(Error)
        status CHAR(1) DEFAULT 'N' NOT NULL,
        
        -- fail_count: ç´€éŒ„å¡«è¡¨å¤±æ•—æ¬¡æ•¸ (æµç¨‹åœ–: å¤±æ•—è¶…é3æ¬¡ -> E)
        fail_count INT DEFAULT 0,
        
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

        -- å»é‡æ©Ÿåˆ¶: åŒä¸€å¤©ã€åŒæ¨™é¡Œçš„æ–°èè¦–ç‚ºé‡è¤‡ï¼Œæ‹’çµ•å¯«å…¥
        UNIQUE KEY unique_news_check (title, publish_date)
    ) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

    -- 3. å»ºç«‹åŸ·è¡Œç´€éŒ„è¡¨ (Optional: ç”¨æ–¼ç´€éŒ„æ¯æ¬¡ Script åŸ·è¡Œç‹€æ³)
    CREATE TABLE IF NOT EXISTS execution_logs (
        id INT AUTO_INCREMENT PRIMARY KEY,
        execution_time DATETIME DEFAULT CURRENT_TIMESTAMP,
        total_processed INT DEFAULT 0,
        success_count INT DEFAULT 0,
        error_count INT DEFAULT 0,
        log_message TEXT
    );

  (4)å»ºç«‹æš«æ™‚çš„ Dockerfile (docker-compose.yml è£¡é¢åƒç…§äº† build: .ï¼Œéœ€è¦ä¸€å€‹ Dockerfile æ‰èƒ½è·‘)
     a.åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„å»ºç«‹ Dockerfileã€‚
     b.è²¼ä¸Šä»¥ä¸‹å…§å®¹ï¼š
       # ä½¿ç”¨ Python 3.9 Slim
        FROM python:3.9-slim

        # è¨­å®šç’°å¢ƒè®Šæ•¸
        ENV PYTHONDONTWRITEBYTECODE=1
        ENV PYTHONUNBUFFERED=1

        # è¨­å®šå·¥ä½œç›®éŒ„
        WORKDIR /app

        # 1. å®‰è£ç³»çµ±ä¾è³´
        # æˆ‘å€‘éœ€è¦ gnupg ä¾†è™•ç†é‡‘é‘°
        RUN apt-get update && apt-get install -y \
            wget \
            gnupg \
            unzip \
            curl \
            # Chrome åŸ·è¡Œæ‰€éœ€ä¾è³´ (Debian 12/13 é©ç”¨)
            libnss3 \
            libxss1 \
            libasound2 \
            fonts-liberation \
            libnspr4 \
            xdg-utils \
            libgbm1 \
            libu2f-udev \
            libvulkan1 \
            && rm -rf /var/lib/apt/lists/*

        # 2. å®‰è£ Google Chrome (ä½¿ç”¨æ–°ç‰ˆ signed-by æ©Ÿåˆ¶ï¼Œä¸ä½¿ç”¨ apt-key)
        RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | \
            gpg --dearmor -o /usr/share/keyrings/google-chrome-keyring.gpg \
            && echo "deb [arch=amd64 signed-by=/usr/share/keyrings/google-chrome-keyring.gpg] http://dl.google.com/linux/chrome/deb/ stable main" \
            > /etc/apt/sources.list.d/google-chrome.list \
            && apt-get update \
            && apt-get install -y google-chrome-stable

        # 3. å®‰è£ Python å¥—ä»¶
        COPY requirements.txt .
        RUN pip install --no-cache-dir -r requirements.txt

        # 4. è¤‡è£½ç¨‹å¼ç¢¼
        COPY . .

        # 5. é è¨­æŒ‡ä»¤
        CMD ["tail", "-f", "/dev/null"]

     c.å•Ÿå‹• Docker Desktop

     d.æ‰“é–‹çµ‚ç«¯æ©Ÿï¼Œé‡è¨­é€£ç·š
       docker context use default

     e.æ‰“é–‹çµ‚ç«¯æ©Ÿï¼Œå˜—è©¦é€£ç·š
       docker ps

     f.æ‰“é–‹çµ‚ç«¯æ©Ÿï¼ŒåŸ·è¡Œæœƒè‡ªå‹•å»ºç½®åŒ…å« Chrome çš„ Python ç’°å¢ƒä»¥åŠåˆå§‹åŒ– MySQL è³‡æ–™åº«
       docker-compose up -d --build

     g.æ‰“é–‹çµ‚ç«¯æ©Ÿï¼ŒæŸ¥çœ‹ç¾åœ¨çš„ç‹€æ…‹
       docker ps
       
       æƒ…æ³ Aï¼šçœ‹åˆ°ç©ºè•©è•©çš„æ¨™é¡Œï¼Œæˆ–æ˜¯ä»€éº¼éƒ½æ²’æœ‰
       CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS   PORTS   NAMES
       
       æƒ…æ³ Bï¼šçœ‹åˆ°å…©è¡Œè³‡æ–™ (mysql å’Œ python)ï¼Œä¾‹å¦‚:
       CONTAINER ID   IMAGE              ...   STATUS          NAMES
       abc12345       mysql:8.0          ...   Up 5 seconds    asus_news_db
       def67890       asus-news-app      ...   Up 5 seconds    asus_news_worker

       â€»å¦‚æœæœ‰æƒ…æ³Aï¼Œå¯èƒ½æ˜¯Dockerfileä¸­çš„ä¾è³´æˆ–é©—è­‰ç‰ˆæœ¬å•é¡Œï¼Œå°‡Dockerfileé‡æ–°ç¢ºèªå¾Œï¼Œå†é‡æ–°åš
         å¼·åˆ¶é‡æ–°ä¸‹è¼‰æ‰€æœ‰çš„ Chrome å’Œä¾è³´å¥—ä»¶ï¼Œç¢ºä¿ä½ çš„æ–° Dockerfile é‚è¼¯è¢«çœŸæ­£åŸ·è¡Œã€‚
         docker-compose build --no-cache
         å»ºç½®æˆåŠŸå¾Œï¼ˆæ²’æœ‰å ±éŒ¯ï¼‰ï¼Œå†å•Ÿå‹•å®ƒ
         docker-compose up -d --force-recreate

================================================================================================
ğŸš€ æ’°å¯«çˆ¬èŸ²ç¨‹å¼å»ºç½® (Windows é–‹ç™¼ç’°å¢ƒ)

ç¬¬ 1 éšæ®µï¼šå»ºç«‹å°ˆæ¡ˆçµæ§‹
1.åœ¨æ ¹ç›®éŒ„ä¸‹å»ºç«‹app è³‡æ–™å¤¾

2.åœ¨appè³‡æ–™å¤¾ä¸‹ï¼Œå»ºç«‹ä»¥ä¸‹æª”æ¡ˆ
    asus-news/
    â”œâ”€â”€ app/                    # æ ¸å¿ƒæ‡‰ç”¨ç¨‹å¼é‚è¼¯
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ main.py             # ç¨‹å¼é€²å…¥é» (Entry Point)
    â”‚   â”œâ”€â”€ config.py           # è¨­å®šæª”è®€å– (è®€å– env)
    â”‚   â”œâ”€â”€ scraper.py          # çˆ¬èŸ²é‚è¼¯ (Requests/BS4)
    â”‚   â”œâ”€â”€ utils.py            # å·¥å…·åŒ…
    â”‚   â”œâ”€â”€ database.py         # è³‡æ–™åº«æ“ä½œ (MySQL é€£ç·šèˆ‡ CRUD)
    â”‚   â”œâ”€â”€ form_filler.py      # è‡ªå‹•å¡«è¡¨é‚è¼¯ (Selenium)
    â”‚â”€â”€ â””â”€â”€ logger.py           # æ—¥èªŒè¨­å®š (Logging)

------------------------------------------------------------------------------------------------
ç¬¬ 2 éšæ®µï¼šæ’°å¯«ç¨‹å¼ç¢¼

1.è«‹å°‡ä»¥ä¸‹å…§å®¹è¤‡è£½åˆ° app/scraper.py
  import logging
  import time
  import random
  from typing import List, Dict
  from selenium import webdriver
  from selenium.webdriver.chrome.service import Service
  from selenium.webdriver.chrome.options import Options
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC
  from webdriver_manager.chrome import ChromeDriverManager

  # è¨­å®š Log æ ¼å¼
  logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
  logger = logging.getLogger(__name__)

  class NewsScraper:
        def __init__(self):
            self.driver = self._setup_driver()

        def _setup_driver(self) -> webdriver.Chrome:
            chrome_options = Options()
            
            # --- [é—œéµä¿®æ­£] é‡å° Docker ç’°å¢ƒçš„æœ€ä½³åŒ–åƒæ•¸ ---
            # ä½¿ç”¨æ–°ç‰ˆ headless æ¨¡å¼ (æ¯”èˆŠç‰ˆæ›´ç©©å®š)
            chrome_options.add_argument("--headless=new")
            
            # è§£æ±º Docker å…±äº«è¨˜æ†¶é«”ä¸è¶³å°è‡´çš„å´©æ½°
            chrome_options.add_argument("--disable-dev-shm-usage")
            
            # è§£æ±º Linux root æ¬Šé™å•é¡Œ
            chrome_options.add_argument("--no-sandbox")
            
            # ç¦ç”¨ GPU (Linux ä¼ºæœå™¨é€šå¸¸æ²’é¡¯å¡)
            chrome_options.add_argument("--disable-gpu")
            
            # è¨­å®šå›ºå®šè¦–çª—å¤§å°ï¼Œé¿å… RWD é€ æˆå…ƒç´ ä½ç½®è·‘æ‰
            chrome_options.add_argument("--window-size=1920,1080")
            
            # å¢åŠ ç©©å®šæ€§çš„é¡å¤–åƒæ•¸
            chrome_options.add_argument("--remote-debugging-port=9222")
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--disable-infobars")
            chrome_options.add_argument("--disable-notifications")
            
            # å½è£ User-Agent (é¿å…è¢« Google èªå®šç‚ºæ©Ÿå™¨äºº)
            chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

            logger.info("æ­£åœ¨åˆå§‹åŒ– Chrome Driver (v2)...")
            
            # è‡ªå‹•å®‰è£ä¸¦è¨­å®š Driver
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
            return driver

        def scrape_google_news(self, keyword: str = "ASUS router security") -> List[Dict]:
            results = []
            try:
                # åŠ å…¥æ™‚é–“ç¯©é¸åƒæ•¸ &tbs=qdr:m6 (æœ€è¿‘ 6 å€‹æœˆ)
                url = f"https://www.google.com/search?q={keyword}&tbm=nws&tbs=qdr:m6"
                logger.info(f"å‰å¾€ URL: {url}")
                
                self.driver.get(url)
                
                # éš¨æ©Ÿå»¶é²ï¼Œæ¨¡æ“¬äººé¡é–±è®€ (Anti-Scraping)
                sleep_time = random.uniform(2, 5)
                logger.info(f"éš¨æ©Ÿå»¶é²: ç­‰å¾… {sleep_time:.2f} ç§’...")
                time.sleep(sleep_time)

                # ç­‰å¾…æ–°èå€å¡Šè¼‰å…¥ (æœ€å¤šç­‰ 15 ç§’)
                WebDriverWait(self.driver, 15).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "div.SoaBEf"))
                )

                articles = self.driver.find_elements(By.CSS_SELECTOR, "div.SoaBEf")
                logger.info(f"æ‰¾åˆ° {len(articles)} ç¯‡ç›¸é—œæ–°è")

                for article in articles:
                    try:
                        title_elem = article.find_element(By.CSS_SELECTOR, "div[role='heading']")
                        link_elem = article.find_element(By.TAG_NAME, "a")
                        date_elem = article.find_element(By.CSS_SELECTOR, ".OSrXXb span")
                        
                        # å˜—è©¦æŠ“å–æ‘˜è¦
                        try:
                            desc_elem = article.find_element(By.CSS_SELECTOR, ".GI74Re")
                            description = desc_elem.text
                        except:
                            description = ""

                        title = title_elem.text
                        link = link_elem.get_attribute("href")
                        date_str = date_elem.text

                        # ç°¡å–®éæ¿¾
                        if "ASUS" in title.upper() or "è¯ç¢©" in title:
                            results.append({
                                "title": title,
                                "url": link,
                                "date_raw": date_str,
                                "source": "Google News",
                                "description": description
                            })
                    except Exception as e:
                        # å–®ç¯‡å¤±æ•—ä¸å½±éŸ¿æ•´é«”
                        continue

            except Exception as e:
                logger.error(f"çˆ¬èŸ²åŸ·è¡ŒæœŸé–“ç™¼ç”ŸéŒ¯èª¤: {e}")
                # å¦‚æœæ˜¯ Timeoutï¼Œå¯èƒ½æ˜¯è¢« Google æ“‹äº†ï¼Œå»ºè­°ä¿ç•™æˆªåœ– (é€²éšåŠŸèƒ½)
                # self.driver.save_screenshot("error_screenshot.png")
                
            finally:
                try:
                    self.driver.quit()
                    logger.info("ç€è¦½å™¨å·²é—œé–‰")
                except:
                    pass
            
            return results

2.è«‹å°‡ä»¥ä¸‹å…§å®¹è¤‡è£½åˆ° app/utils.pyï¼Œå»ºç«‹æ—¥æœŸè™•ç†å·¥å…·
    import re
    from datetime import datetime, timedelta

    def parse_relative_date(date_str: str) -> str:
        """
        å°‡ Google News çš„ç›¸å°æ™‚é–“ (e.g., '3 å¤©å‰', '1 é€±å‰') 
        è½‰æ›ç‚ºæ¨™æº–æ—¥æœŸæ ¼å¼ (YYYY-MM-DD)ã€‚
        """
        today = datetime.now()
        
        try:
            # å»é™¤å‰å¾Œç©ºç™½
            date_str = date_str.strip()

            # è™•ç† "2025å¹´5æœˆ28æ—¥" é€™ç¨®çµ•å°æ—¥æœŸ
            if "å¹´" in date_str and "æœˆ" in date_str:
                dt = datetime.strptime(date_str, "%Yå¹´%mæœˆ%dæ—¥")
                return dt.strftime("%Y-%m-%d")

            # è™•ç† "X å¤©å‰"
            days_match = re.search(r'(\d+)\s*å¤©å‰', date_str)
            if days_match:
                days = int(days_match.group(1))
                dt = today - timedelta(days=days)
                return dt.strftime("%Y-%m-%d")

            # è™•ç† "X é€±å‰"
            weeks_match = re.search(r'(\d+)\s*é€±å‰', date_str)
            if weeks_match:
                weeks = int(weeks_match.group(1))
                dt = today - timedelta(weeks=weeks)
                return dt.strftime("%Y-%m-%d")

            # è™•ç† "X å°æ™‚å‰" (è¦–ç‚ºä»Šå¤©)
            hours_match = re.search(r'(\d+)\s*å°æ™‚å‰', date_str)
            if hours_match:
                return today.strftime("%Y-%m-%d")
                
            # è™•ç† "æ˜¨å¤©"
            if "æ˜¨å¤©" in date_str:
                dt = today - timedelta(days=1)
                return dt.strftime("%Y-%m-%d")

            # è‹¥éƒ½ç„¡æ³•è§£æï¼Œå›å‚³ä»Šå¤© (æˆ–ä½ å¯ä»¥é¸æ“‡æ‹‹å‡ºéŒ¯èª¤)
            return today.strftime("%Y-%m-%d")

        except Exception as e:
            print(f"æ—¥æœŸè§£æå¤±æ•—: {date_str}, éŒ¯èª¤: {e}")
            return today.strftime("%Y-%m-%d")

3.è«‹å°‡ä»¥ä¸‹å…§å®¹è¤‡è£½åˆ° app/database.pyï¼Œå»ºç«‹é€£ç·šè³‡æ–™åº«åŠå¯«å…¥
    import mysql.connector
    import os
    import logging
    from typing import List, Dict, Optional

    # è¨­å®š logger
    logger = logging.getLogger(__name__)

    class Database:
        def __init__(self):
            # å¾ç’°å¢ƒè®Šæ•¸è®€å–é€£ç·šè³‡è¨Š (Docker Compose è£¡è¨­å®šçš„)
            self.config = {
                'user': os.getenv('DB_USER', 'scraper_user'),
                'password': os.getenv('DB_PASSWORD', 'scraper_password'),
                'host': os.getenv('DB_HOST', 'mysql-db'),
                'database': os.getenv('DB_NAME', 'security_news'),
                'raise_on_warnings': False,
                'autocommit': False # æˆ‘å€‘æ‰‹å‹• commit ä»¥ç¢ºä¿äº¤æ˜“å®Œæ•´æ€§
            }

        def get_connection(self):
            """å»ºç«‹ä¸¦å›å‚³è³‡æ–™åº«é€£ç·š"""
            return mysql.connector.connect(**self.config)

        def insert_news(self, news_list: List[Dict]) -> int:
            """
            æµç¨‹åœ–æ­¥é©Ÿ 3 & 4: å¯«å…¥è³‡æ–™ä¸¦å»é‡
            - ä½¿ç”¨ INSERT IGNORE å¿½ç•¥å·²å­˜åœ¨çš„ (title + publish_date)
            - é è¨­ status ç‚º 'N'
            """
            if not news_list:
                return 0

            inserted_count = 0
            conn = None
            cursor = None
            
            try:
                conn = self.get_connection()
                cursor = conn.cursor()

                # SQL èªæ³•: è‹¥è¤‡åˆéµé‡è¤‡å‰‡å¿½ç•¥ï¼Œå¦å‰‡æ’å…¥æ–°è³‡æ–™
                sql = """
                INSERT IGNORE INTO news (title, url, publish_date, source, description, status, fail_count)
                VALUES (%s, %s, %s, %s, %s, 'N', 0)
                """

                for item in news_list:
                    val = (
                        item['title'],
                        item['url'],
                        item['publish_date'],
                        item['source'],
                        item.get('description', '')  # å–å¾—æ‘˜è¦ï¼Œè‹¥ç„¡å‰‡ç‚ºç©ºå­—ä¸²
                    )
                    cursor.execute(sql, val)
                    
                    # æª¢æŸ¥é€™ç­†æ˜¯å¦çœŸçš„å¯«å…¥ (rowcount > 0 ä»£è¡¨æˆåŠŸæ’å…¥ï¼Œ0 ä»£è¡¨è¢« IGNORE)
                    if cursor.rowcount > 0:
                        inserted_count += 1
                    else:
                        logger.debug(f"Duplicate found (Skipped): {item['title'][:30]}...")

                conn.commit()
                logger.info(f"[DB] æ‰¹æ¬¡ä½œæ¥­çµæŸ: è¼¸å…¥ {len(news_list)} ç­† -> å¯¦éš›æ–°å¢ {inserted_count} ç­† (é‡è¤‡ {len(news_list)-inserted_count} ç­†)")

            except mysql.connector.Error as err:
                logger.error(f"[DB Error] å¯«å…¥å¤±æ•—: {err}")
                if conn:
                    conn.rollback()
            finally:
                if cursor: cursor.close()
                if conn and conn.is_connected(): conn.close()
            
            return inserted_count

        def get_pending_news(self) -> List[Dict]:
            """
            æµç¨‹åœ–æ­¥é©Ÿ 5: ç²å–æ‰€æœ‰ç‹€æ…‹ç‚º 'N' (New) çš„è³‡æ–™ï¼Œæº–å‚™é€²è¡Œå¡«è¡¨
            """
            conn = None
            cursor = None
            results = []
            try:
                conn = self.get_connection()
                cursor = conn.cursor(dictionary=True) # å›å‚³å­—å…¸æ ¼å¼ï¼Œæ–¹ä¾¿å­˜å–æ¬„ä½
                
                # ä¾ç…§æ—¥æœŸæ’åºï¼ŒèˆŠçš„æ–°èå…ˆè™•ç†
                sql = "SELECT * FROM news WHERE status = 'N' ORDER BY publish_date ASC"
                cursor.execute(sql)
                results = cursor.fetchall()
                
            except mysql.connector.Error as err:
                logger.error(f"[DB Error] è®€å–å¾…è™•ç†è³‡æ–™å¤±æ•—: {err}")
            finally:
                if cursor: cursor.close()
                if conn and conn.is_connected(): conn.close()
            return results

        def update_status(self, news_id: int, status: str):
            """
            æµç¨‹åœ–æ­¥é©Ÿ 7: å¡«è¡¨æˆåŠŸå¾Œï¼Œæ›´æ–°ç‹€æ…‹ (ä¾‹å¦‚è®Šæ›´ç‚º 'Y')
            """
            conn = None
            cursor = None
            try:
                conn = self.get_connection()
                cursor = conn.cursor()
                
                sql = "UPDATE news SET status = %s WHERE id = %s"
                cursor.execute(sql, (status, news_id))
                conn.commit()
                logger.info(f"[DB] æ–°è ID {news_id} ç‹€æ…‹æ›´æ–°ç‚º: '{status}'")
                
            except mysql.connector.Error as err:
                logger.error(f"[DB Error] æ›´æ–°ç‹€æ…‹å¤±æ•—: {err}")
            finally:
                if cursor: cursor.close()
                if conn and conn.is_connected(): conn.close()

        def record_failure(self, news_id: int):
            """
            æµç¨‹åœ–å¤±æ•—è¿´åœˆé‚è¼¯: 
            1. å¤±æ•—æ¬¡æ•¸ (fail_count) + 1
            2. è‹¥å¤±æ•—æ¬¡æ•¸ >= 3ï¼Œå°‡ status è¨­ç‚º 'E' (Error/æ”¾æ£„)
            """
            conn = None
            cursor = None
            try:
                conn = self.get_connection()
                cursor = conn.cursor()
                
                # 1. å¢åŠ å¤±æ•—æ¬¡æ•¸
                sql_update = "UPDATE news SET fail_count = fail_count + 1 WHERE id = %s"
                cursor.execute(sql_update, (news_id,))
                
                # 2. æª¢æŸ¥ç›®å‰å¤±æ•—æ¬¡æ•¸
                sql_check = "SELECT fail_count FROM news WHERE id = %s"
                cursor.execute(sql_check, (news_id,))
                result = cursor.fetchone()
                
                if result:
                    current_fail_count = result[0]
                    logger.warning(f"[DB] æ–°è ID {news_id} å¤±æ•—æ¬¡æ•¸å¢åŠ ç‚º: {current_fail_count}")
                    
                    # 3. åˆ¤æ–·æ˜¯å¦è¶…éé–¾å€¼ (ä¾‹å¦‚ 3 æ¬¡)
                    if current_fail_count >= 3:
                        sql_mark_error = "UPDATE news SET status = 'E' WHERE id = %s"
                        cursor.execute(sql_mark_error, (news_id,))
                        logger.error(f"[DB] æ–°è ID {news_id} å¤±æ•—æ¬¡æ•¸éå¤š (>=3)ï¼Œæ¨™è¨˜ç‚º Error (E)")
                
                conn.commit()
                
            except mysql.connector.Error as err:
                logger.error(f"[DB Error] ç´€éŒ„å¤±æ•—æ¬¡æ•¸éŒ¯èª¤: {err}")
            finally:
                if cursor: cursor.close()
                if conn and conn.is_connected(): conn.close()

4.è«‹å°‡ä»¥ä¸‹å…§å®¹è¤‡è£½åˆ° app/main.pyï¼Œæ›´æ–°ä¸»ç¨‹å¼
    import logging
    import time
    import random
    import os
    from datetime import datetime  # <--- [æ–°å¢] ç”¨æ–¼ç´€éŒ„æ“·å–æ™‚é–“
    from scraper import NewsScraper
    from database import Database
    from utils import parse_relative_date
    from form_filler import FormFiller  # <--- [é‡è¦] å¼•å…¥å¡«è¡¨æ¨¡çµ„

    # è¨­å®šå…¨åŸŸ Log æ ¼å¼
    logging.basicConfig(
        level=logging.INFO, 
        format='%(asctime)s - [%(levelname)s] - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    logger = logging.getLogger(__name__)

    def process_scraping_job():
        """
        [Phase 1] çˆ¬èŸ²èˆ‡å…¥åº«æµç¨‹
        å°æ‡‰æµç¨‹åœ–: çˆ¬èŸ² -> è³‡æ–™æ¸…æ´— -> å¯«å…¥è³‡æ–™åº«(åˆ¤æ–·æ˜¯å¦å­˜åœ¨)
        """
        logger.info("=== éšæ®µä¸€: å•Ÿå‹•çˆ¬èŸ²ä½œæ¥­ ===")
        
        # 1. åˆå§‹åŒ–çˆ¬èŸ²
        scraper = NewsScraper()
        keyword = "ASUS router security"
        
        # 2. åŸ·è¡Œçˆ¬å–
        logger.info(f"æ­£åœ¨æœå°‹é—œéµå­—: {keyword}")
        raw_data = scraper.scrape_google_news(keyword)
        
        if not raw_data:
            logger.warning("æœ¬æ¬¡æœªæŠ“å–åˆ°ä»»ä½•è³‡æ–™ï¼Œè·³éå…¥åº«æµç¨‹ã€‚")
            return

        # 3. è³‡æ–™æ¸…æ´— (Data Cleaning) - [é‡å°æµç¨‹åœ–éœ€æ±‚å¼·åŒ–]
        logger.info("æ­£åœ¨æ¸…æ´—è³‡æ–™æ ¼å¼ (æ—¥æœŸæ¨™æº–åŒ– & å»é™¤ç©ºç™½)...")
        cleaned_data = []
        
        # [æ–°å¢] çµ±ä¸€è¨­å®šæœ¬æ¬¡æ‰¹æ¬¡çš„ã€Œæ“·å–æ™‚é–“ã€
        capture_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        for item in raw_data:
            # [æ¸…æ´— 1] æ—¥æœŸæ¨™æº–åŒ–: "3 å¤©å‰" -> "2023-12-05"
            std_date = parse_relative_date(item['date_raw'])
            
            # [æ¸…æ´— 2] æ–‡å­—æ¸…æ´—: ç§»é™¤å‰å¾Œç©ºç™½/æ›è¡Œ (ç¬¦åˆæµç¨‹åœ– "ç§»é™¤ç©ºç™½" è¦æ±‚)
            clean_title = item['title'].strip()
            clean_desc = item.get('description', '').strip()
            
            # æ•´ç†ç¬¦åˆè¦æ±‚çš„å…­å¤§æ¬„ä½:
            # 1. ä¾†æºç¶²ç«™ (source)
            # 2. æ¨™é¡Œ (title)
            # 3. ç™¼å¸ƒæ—¥æœŸ (publish_date)
            # 4. å…§æ–‡æ‘˜è¦/é‡é» (description)
            # 5. åŸå§‹é€£çµ (url)
            # 6. æ“·å–æ™‚é–“ (captured_at) - [æ–°å¢]
            cleaned_data.append({
                'title': clean_title,
                'url': item['url'],
                'publish_date': std_date,
                'source': item['source'],
                'description': clean_desc,
                'captured_at': capture_time 
            })

        # 4. å¯«å…¥è³‡æ–™åº« (Insert & Deduplicate)
        db = Database()
        new_count = db.insert_news(cleaned_data)
        logger.info(f"éšæ®µä¸€çµæŸã€‚è³‡æ–™åº«æ–°å¢: {new_count} ç­†ã€‚")


    def process_form_filling_job():
        """
        [Phase 2] è‡ªå‹•å¡«è¡¨æµç¨‹
        å°æ‡‰æµç¨‹åœ–: æª¢æŸ¥ç‹€æ…‹ 'N' -> å¡«å¯« Google è¡¨å–® -> æˆåŠŸæ›´æ–° 'Y' / å¤±æ•—è¨˜æ•¸
        """
        logger.info("=== éšæ®µäºŒ: æª¢æŸ¥å¾…å¡«å¯«è³‡æ–™ (Status='N') ===")
        db = Database()
        
        # 5. å¾ DB æ’ˆå‡ºæ‰€æœ‰ Status = 'N' çš„è³‡æ–™
        pending_tasks = db.get_pending_news()
        
        if not pending_tasks:
            logger.info("æ²’æœ‰æ–°è³‡æ–™éœ€è¦å¡«å¯« (All caught up)ã€‚")
            return

        logger.info(f"ç™¼ç¾ {len(pending_tasks)} ç­†å¾…è™•ç†ä»»å‹™ï¼Œæº–å‚™é–‹å§‹å¡«è¡¨...")

        # åˆå§‹åŒ–å¡«è¡¨å™¨ (å»ºè­°åœ¨è¿´åœˆå¤–åˆå§‹åŒ– driverï¼Œé€™è£¡ç‚ºæ±‚ç°¡å–®æ¯æ¬¡é‡å•Ÿ)
        # è‹¥è¦å„ªåŒ–æ•ˆèƒ½ï¼Œå¯å°‡ FormFiller æ”¾åœ¨è¿´åœˆå¤–ï¼Œä½†éœ€ç¢ºä¿å®ƒèƒ½è™•ç†å¤šç­†æäº¤
        
        for task in pending_tasks:
            news_id = task['id']
            title = task['title']
            logger.info(f"æ­£åœ¨è™•ç†ä»»å‹™ ID:{news_id} | æ¨™é¡Œ: {title[:20]}...")

            filler = None
            try:
                # --- 6. åŸ·è¡Œå¡«è¡¨é‚è¼¯ (æ­£å¼ç‰ˆ) ---
                filler = FormFiller() # åˆå§‹åŒ–ç€è¦½å™¨
                is_success = filler.fill_form(task) # åŸ·è¡Œè‡ªå‹•å¡«å¯«
                
                if is_success:
                    # 7. æˆåŠŸæµç¨‹: æ›´æ–°ç‹€æ…‹ç‚º 'Y'
                    db.update_status(news_id, 'Y')
                    logger.info(f"-> ä»»å‹™æˆåŠŸ (ID {news_id})")
                else:
                    # å¤±æ•—æµç¨‹
                    raise Exception("Google è¡¨å–®æäº¤é©—è­‰å¤±æ•— (æ‰¾ä¸åˆ°æˆåŠŸè¨Šæ¯)")

            except Exception as e:
                logger.error(f"-> ä»»å‹™å¤±æ•— (ID {news_id}): {e}")
                # å¤±æ•—è¿´åœˆ: è¨˜æ•¸ +1ï¼Œè‹¥è¶…é 3 æ¬¡å‰‡æ¨™è¨˜ç‚º 'E'
                db.record_failure(news_id)
            finally:
                # ç¢ºä¿æ¯æ¬¡å¡«å®Œéƒ½é—œé–‰ç€è¦½å™¨ (é¿å…è¨˜æ†¶é«”æ´©æ¼)
                if filler and hasattr(filler, 'driver'):
                    try:
                        filler.driver.quit()
                    except:
                        pass

    def main():
        try:
            # ç‚ºäº†ç¢ºä¿ DB å®¹å™¨å·²å®Œå…¨å•Ÿå‹•
            time.sleep(2)
            
            # åŸ·è¡Œå®Œæ•´å·¥ä½œæµ
            process_scraping_job()
            process_form_filling_job()
            
            logger.info("=== æ‰€æœ‰è‡ªå‹•åŒ–ä½œæ¥­åŸ·è¡Œå®Œç•¢ ===")
            
        except Exception as e:
            logger.critical(f"ä¸»ç¨‹å¼ç™¼ç”Ÿæœªé æœŸå´©æ½°: {e}")

    if __name__ == "__main__":
        main()

5.åœ¨ Docker è£¡é¢æ¸¬è©¦çˆ¬èŸ²
  docker exec -it asus_news_worker python app/main.py
  æ‡‰è©²æœƒçœ‹åˆ° Log é¡¯ç¤ºé¡ä¼¼ï¼š [DB] æ‰¹æ¬¡ä½œæ¥­çµæŸ: è¼¸å…¥ 9 ç­† -> å¯¦éš›æ–°å¢ 8 ç­† (é‡è¤‡ 1 ç­†)
  é€™å°±ä»£è¡¨é‚£ 8 ç­†æˆåŠŸå¯«å…¥ï¼Œè€Œé‡è¤‡çš„ 1 ç­†è¢«å®‰å…¨åœ°å¿½ç•¥äº†ã€‚

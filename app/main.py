import logging
import time
import os
import gc
import threading  # æ–°å¢: ç”¨æ–¼è¨ˆæ™‚å™¨
import sys        # æ–°å¢: ç”¨æ–¼å¼·åˆ¶é€€å‡º
from datetime import datetime, timedelta, timezone
from scraper import NewsScraper
from database import Database
from utils import parse_relative_date
from form_filler import FormFiller
from logger import logger

# --- è¨­å®šé€¾æ™‚æ™‚é–“ (ç§’) ---
# è¨­å®šç‚º 300 ç§’ (5åˆ†é˜)ï¼Œå¦‚æœè¶…éé€™å€‹æ™‚é–“é‚„æ²’è·‘å®Œï¼Œè¦–ç‚ºå¡æ­»
JOB_TIMEOUT_SECONDS = 300 

# å¤šæºæœå°‹è¨­å®š
SEARCH_CONFIGS = [
    {
        "category": "Google News (EN)",
        "query": "ASUS router security",
        "type": "news",
        "lang": "en"
    },
    {
        "category": "Google News (TW)",
        "query": "è¯ç¢© è·¯ç”±å™¨ è³‡å®‰",
        "type": "news",
        "lang": "zh-TW"
    },
    {
        "category": "å®˜æ–¹è³‡æº",
        "query": "site:asus.com security router",
        "type": "web",
        "lang": "en"
    },
    {
        "category": "è³‡å®‰é€šå ±", 
        "query": "site:bleepingcomputer.com OR site:thehackernews.com ASUS",
        "type": "news",
        "lang": "en"
    }
]

def force_exit_handler():
    """
    ç•¶è¶…æ™‚ç™¼ç”Ÿæ™‚çš„è™•ç†å‡½å¼ã€‚
    ç›´æ¥ä½¿ç”¨ os._exit(1) å¼·åˆ¶æ®ºæ­»æ‰€æœ‰åŸ·è¡Œç·’èˆ‡é€²ç¨‹ã€‚
    """
    logger.critical(f"âš ï¸ åµæ¸¬åˆ°ä»»å‹™åŸ·è¡Œè¶…é {JOB_TIMEOUT_SECONDS} ç§’ï¼Œåˆ¤å®šç‚ºå¡æ­»ã€‚")
    logger.critical("ğŸ’€ æ­£åœ¨å¼·åˆ¶çµæŸç¨‹å¼ (Force Kill)ï¼Œç­‰å¾… Docker è‡ªå‹•é‡å•Ÿ...")
    # os._exit ä¸æœƒè§¸ç™¼æ¸…ç† (finally)ï¼Œæ˜¯ç›®å‰è§£æ±º Driver å¡æ­»çš„å”¯ä¸€æ‰‹æ®µ
    os._exit(1)

def process_scraping_job():
    logger.info("=== éšæ®µä¸€: é›™èªå¤šæºçˆ¬èŸ²å•Ÿå‹• ===")
    scraper = NewsScraper()
    
    try:
        all_news_data = []
        
        for config in SEARCH_CONFIGS:
            logger.info(f"åŸ·è¡Œä»»å‹™: {config['category']}...")
            
            raw_data = scraper.scrape_google_search(
                query=config['query'],
                source_category=config['category'],
                search_type=config['type'],
                lang=config['lang'] 
            )
            
            all_news_data.extend(raw_data[:5])
            time.sleep(2)

        if not all_news_data:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•è³‡æ–™ã€‚")
            return

        logger.info(f"æœå°‹å®Œæˆï¼Œå…± {len(all_news_data)} ç­†ï¼Œé–‹å§‹é–±è®€å…§æ–‡...")
        
        cleaned_data = []
        tw_tz = timezone(timedelta(hours=8))
        capture_time = datetime.now(tw_tz).strftime('%Y-%m-%d %H:%M:%S')

        for item in all_news_data:
            deep_content = scraper.read_article_content(item['url'])
            
            if deep_content in ["SKIP_404", "SKIP_PDF", "SKIP_ERROR","drifted off-grid","Page Not Found!","SORRY","Sorry! Page not found"]:
                logger.warning(f"è·³éç„¡æ•ˆ/éŒ¯èª¤é€£çµ: {item['title'][:20]}...")
                continue
            
            final_desc = "ç„¡æ‘˜è¦"
            if deep_content and len(deep_content) > 30 and "å¤±æ•—" not in deep_content:
                final_desc = deep_content
            elif item.get('description'):
                final_desc = f"[Googleæ‘˜è¦] {item['description']}"
            
            std_date = parse_relative_date(item['date_raw'])
            
            cleaned_data.append({
                # ç¢ºä¿å»ç©ºç™½
                'title': item['title'].strip(),
                'url': item['url'],
                'publish_date': std_date,
                'source': item['source'],
                'description': final_desc,
                'captured_at': capture_time 
            })

        if cleaned_data:
            db = Database()
            new_count = db.insert_news(cleaned_data)
            logger.info(f"éšæ®µä¸€çµæŸã€‚è³‡æ–™åº«å¯¦éš›æ–°å¢: {new_count} ç­†ã€‚")
        else:
            logger.warning("éšæ®µä¸€çµæŸã€‚æ²’æœ‰æœ‰æ•ˆè³‡æ–™å¯å¯«å…¥ã€‚")
        
    except Exception as e:
        logger.error(f"çˆ¬èŸ²éšæ®µç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        scraper.close()
        del scraper
        gc.collect()

def process_form_filling_job():
    logger.info("=== éšæ®µäºŒ: å¡«å¯«è¡¨å–® (Status='N') ===")
    db = Database()
    pending_tasks = db.get_pending_news()
    
    # çµ±è¨ˆè®Šæ•¸
    total_tasks = 0
    success_count = 0
    fail_count = 0
    
    if not pending_tasks:
        logger.info("æ²’æœ‰å¾…è™•ç†è³‡æ–™ã€‚")
        return total_tasks, success_count, fail_count

    total_tasks = len(pending_tasks)
    logger.info(f"ç™¼ç¾ {total_tasks} ç­†ä»»å‹™ï¼Œå•Ÿå‹•å¡«è¡¨æ©Ÿå™¨äºº...")
    
    for i, task in enumerate(pending_tasks):
        news_id = task['id']
        title = task['title']
        logger.info(f"[{i+1}/{total_tasks}] å¡«å¯«ä¸­: {title[:15]}...")

        filler = None
        try:
            filler = FormFiller()
            is_success = filler.fill_form(task)
            
            if is_success:
                db.update_status(news_id, 'Y')
                logger.info(f"-> æˆåŠŸ (ID {news_id})")
                success_count += 1
            else:
                raise Exception("æäº¤å¤±æ•—")

        except Exception as e:
            logger.error(f"-> å¤±æ•— (ID {news_id}): {e}")
            db.record_failure(news_id)
            fail_count += 1
        finally:
            if filler:
                try: filler.driver.quit()
                except: pass
            del filler
            gc.collect()
            time.sleep(3)
            
    return total_tasks, success_count, fail_count

def run_cycle_with_watchdog():
    """
    åŸ·è¡Œä¸€æ¬¡å®Œæ•´çš„çˆ¬èŸ²èˆ‡å¡«è¡¨å¾ªç’°ï¼Œä¸¦åŠ ä¸Šé€¾æ™‚ç›£æ§ã€‚
    """
    # 1. å•Ÿå‹•è¨ˆæ™‚å™¨ (Watchdog)
    # å¦‚æœé€™å€‹è¨ˆæ™‚å™¨å€’æ•¸çµæŸï¼Œå°±æœƒåŸ·è¡Œ force_exit_handler æ®ºæ­»ç¨‹å¼
    timer = threading.Timer(JOB_TIMEOUT_SECONDS, force_exit_handler)
    timer.start()
    
    try:
        # åŸ·è¡Œä¸»è¦ä»»å‹™
        time.sleep(2)
        process_scraping_job()
        gc.collect()
        time.sleep(2)
        
        total, success, fail = process_form_filling_job()
        
        logger.info("=== å…¨éƒ¨å®Œæˆ ===")
        logger.info(f"åŸ·è¡Œçµ±è¨ˆ: ç¸½å…± {total} ç­† | æˆåŠŸ: {success} ç­† | å¤±æ•—: {fail} ç­†")
        
    finally:
        # 2. ä»»å‹™å¦‚æœæ­£å¸¸çµæŸï¼Œå¿…é ˆå–æ¶ˆè¨ˆæ™‚å™¨ï¼Œå¦å‰‡å®ƒæœƒåœ¨èƒŒæ™¯ç¹¼çºŒå€’æ•¸ç„¶å¾Œæ®ºæ­»ç¨‹å¼
        timer.cancel()

def main():
    logger.info("=== ç³»çµ±å•Ÿå‹•ï¼šé€²å…¥è‡ªå‹•åŒ–æ’ç¨‹æ¨¡å¼ ===")
    
    # while True:
    try:
        # ä½¿ç”¨å¸¶æœ‰ç›£æ§æ©Ÿåˆ¶çš„å‡½å¼ä¾†åŸ·è¡Œä»»å‹™
        run_cycle_with_watchdog()
            
    except Exception as e:
        logger.critical(f"ä¸»ç¨‹å¼å´©æ½° (Exception): {e}")
        # å¦‚æœæ˜¯åš´é‡éŒ¯èª¤ï¼Œä¹Ÿå¯ä»¥é¸æ“‡ç›´æ¥é‡å•Ÿ Docker
        # os._exit(1)
            
    # è¨­å®šä¸‹æ¬¡åŸ·è¡Œçš„ç­‰å¾…æ™‚é–“ (ç›®å‰è¨­å®šç‚º 24 å°æ™‚ = 86400 ç§’)
    # wait_seconds = 86400 
    # logger.info(f"é€²å…¥å¾…æ©Ÿæ¨¡å¼ï¼Œ{wait_seconds/3600} å°æ™‚å¾Œå°‡å†æ¬¡åŸ·è¡Œ...")
    # time.sleep(wait_seconds)

if __name__ == "__main__":
    main()